---
title: "Practical Machine Learning Coursera Project"
author: "Brian"
date: "2022-12-03"
output: 
    html_document: 
      keep_md: yes
---
## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. This project will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they performed the activity. 

```{r setup, echo=TRUE, message=FALSE}
library(knitr)
library(caret)
library(ggplot2)
library(corrplot)
library(lattice)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
library(gbm)
library(data.table)
library(plotly)
knitr::opts_chunk$set(echo = TRUE, dev = "png", cache = TRUE)
```

## Load and Read the Data
```{r Load and Read the Data}
training <- read.csv("pml-training.csv")
field_data <- read.csv("pml-testing.csv")
dim(training)
dim(field_data)
```
Wow! Both data sets contain 160 variables!  The training data set contains 19,622 observations while the field_data set contains 20 observations.  Let's see if we can eliminate any variables that don't contribute to the prediction by cleaning the training data set.

## Cleaning the Data
We'll start by removing columns that do not contain any values.
```{r Remove_NA_Columns}
training <- training[, colSums(is.na(training)) == 0]
dim(training)
```
We can continue by eliminating variables that contain personal information and time stamped data that are not accelerometer data.
```{r Remove_Timestamps}
# Remove variables with time stamps and personal information
classe <- training$classe
training_scrub <- grepl("^X|timestamp|window", names(training))
training <- training[, !training_scrub]
training_assess <- training[, sapply(training, is.numeric)]
training_assess$classe <- classe
dim(training_assess)
```
That's a little more manageable!  The training set now only have 53 variables.  We are ready to slice our training data set.

## Slicing the data
We will split the training data set into training (70% of data) and a validation (30% of data) data sets.
```{r Slice_the_Training_Data_Set}
inTrain <- createDataPartition(training_assess$classe, p=0.70, list=F)
training_data <- training_assess[inTrain, ]
val_data <- training_assess[-inTrain, ]
dim(training_data)
dim(val_data)
```

A visual representation of variable correlation prior to proceeding to modeling the data.
```{r Correlation_Plot}
cp <- cor(training_data[, -length(names(training_data))])
corrplot(cp, order="FPC", method="circle", type="lower",  tl.cex = 0.6, tl.col = rgb(0, 0, 0))
```




## Data Modeling
We will assess three models to the training data set and the best one relative to the validation data set will be used to predict the activity method for the field_data observations. The three modeling methods are Decision Tree, Generalized Boosted Model, and Random Forests.

### Decision Tree
```{r Decision_Tree_Matrix}
DTM <- rpart(classe ~ ., data=training_data, method="class")
fancyRpartPlot(DTM)
```

```{r Predict_with_DTM_Model}
set.seed(123)
predict_DTM <- predict(DTM, newdata=val_data, type="class")
conf_matrix_DTM <- confusionMatrix(table(predict_DTM, val_data$classe))
conf_matrix_DTM
```

### Generalized Boosted Model
```{r Predict_with_GBM_Model}
set.seed(123)
GBM <- trainControl(method="repeatedcv", number = 5, repeats = 2)
modfit_GBM <- train(classe ~., method="gbm", data=training_data, verbose=FALSE)

predict_GBM <- predict(modfit_GBM, newdata=val_data)
conf_matrix_GBM <- confusionMatrix(table(predict_GBM, val_data$classe))
conf_matrix_GBM
```
### Random Forests
```{r RF_Model}
set.seed(123)
RF <- trainControl(method="cv", number=5)
modfit_RF <- train(classe ~ ., data=training_data, method="rf", trControl=RF, ntree=250)
modfit_RF
```




```{r Predict_with_RF_Model}
predictRF <- predict(modfit_RF, val_data)
confusionMatrix(table(val_data$classe, predictRF))
```


## Prediction on the Field_Data Set
Accuracy of the models were,
Decision Tree: 73.93%;
Generalized Boosted Model: 95.90%; and,
Random Forests: 99.10%

```{r Accuracy_and_Error}
predict <- factor(predictRF) 
classe <- factor(val_data$classe)
accuracy <- postResample(predict, classe)
accuracy[1]
oose <- 1-accuracy[1]
oose
```
As the Random Forest model was the most accurate, with an accuracy of **`r format(round(accuracy[1], 4), nsmall=4)`** and an out-of-sample error of **`r format(round(oose, 4), nsmall=4)`**, applying it to the Field_Data set yields:
```{r Results}
result <- predict(modfit_RF, field_data[, -length(names(field_data))])
result
```






