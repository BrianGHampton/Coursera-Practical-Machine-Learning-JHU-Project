---
title: "Practical Machine Learning Coursera Project"
author: "Brian"
date: "2022-12-03"
output: 
    html_document: 
      keep_md: yes
---
## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. This project will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they performed the activity. 

```{r setup, echo=TRUE, message=FALSE}
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(ggcorrplot)
knitr::opts_chunk$set(echo = TRUE, dev = "png", cache = TRUE)
```

## Load and Read the Data
```{r Load and Read the Data}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
dim(training)
dim(testing)
```
Wow! Both data sets contain 160 variables!  The training data set contains 19,622 observations while the testing set contains 20 observations.  Let's see if we can eliminate any variables that don't contribute to the prediction.

## Cleaning the Data
We'll start by removing columns that do not contain any values.
```{r Remove NA Columns}
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
```
Now we will eliminate variables that do not appear to contribute to the accelerometer inputs.
```{r Remove Variables that do not Contribute to Accelerometers}
classe <- training$classe
training_scrub <- grepl("^X|timestamp|window", names(training))
training <- training[, !training_scrub]
training_assess <- training[, sapply(training, is.numeric)]
training_assess$classe <- classe
testing_scrub <- grepl("^X|timestamp|window", names(testing))
testing <- testing[, !testing_scrub]
testing_assess <- testing[, sapply(testing, is.numeric)]
dim(training_assess)
dim(testing_assess)
```
That's a little more manageable!  Both data sets now only have 53 variables.  Now we are ready to slice our training data set.

## Slicing the data
We will split the training_assess data set into a training data set and a validation data set.
```{r Slice the Training Data Set}
set.seed(123) 
inTrain <- createDataPartition(training_assess$classe, p=0.70, list=F)
training_data <- training_assess[inTrain, ]
val_data <- training_assess[-inTrain, ]
```
## Data Modeling
words go here
```{r RF Model}
controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=training_data, method="rf", trControl=controlRf, ntree=250)
modelRf
```
More words about estimating the performance of the model on the validation data set.
```{r Predict with RF Model}
predictRf <- predict(modelRf, val_data)
confusionMatrix(table(val_data$classe, predictRf))

predict <- factor(predictRf) 
classe <- factor(val_data$classe)
accuracy <- postResample(predict, classe)
accuracy
1-accuracy[1]
```
Words about accuracy and out-of-sample error.

## Predicting on the Test Data Set
```{r Results}
result <- predict(modelRf, testing_assess[, -length(names(testing_assess))])
result
```
And finally a correlation plot and a decision tree matrix.
```{r Correlation Plot}
cp <- cor(training_data[, -length(names(training_data))])
ggcorrplot(cp, title="Correlation Plot from Accelerometer Data", tl.cex=6)
```

```{r Decision Tree Matrix}
DTM <- rpart(classe ~ ., data=training_data, method="class")
plot(DTM, main="Decision Tree Matrix from Accelerometer Data")
text(DTM, cex=0.75)
```





